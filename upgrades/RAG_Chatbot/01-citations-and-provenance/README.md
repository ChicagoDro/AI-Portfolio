````markdown
# Upgrade 01 – Add Citations & Provenance

This upgrade adds **source tracking** to the Chitown Custom Choppers RAG bot:

1. Use `similarity_search_with_score` instead of plain `similarity_search`
2. Track `file_name`, `page`, `document_type`, and similarity **score**
3. Make the LCEL chain return `{"answer": ..., "sources": [...]}`  
4. Render a **Sources** section in Streamlit under each answer

Below are the key code changes.

---

## 1️⃣ Use `similarity_search_with_score`

**Before (conceptually):**

```python
retriever = loaded_vectorstore.as_retriever(search_kwargs={"k": 3})
docs = retriever.invoke(query)
````

**After:**

We call FAISS directly so we can get both documents **and** scores:

```python
from langchain_core.documents import Document
from typing import List, Dict, Any, Tuple

def retrieve_with_scores(inputs: Dict[str, Any]) -> Dict[str, Any]:
    query = inputs["query"]
    classification = inputs["classification"]

    metadata_filter = build_metadata_filter(classification)

    search_kwargs: Dict[str, Any] = {"k": 3}
    if metadata_filter:
        search_kwargs["filter"] = metadata_filter

    docs_and_scores: List[Tuple[Document, float]] = (
        loaded_vectorstore.similarity_search_with_score(
            query,
            **search_kwargs,
        )
    )

    ...
```

Now we have `[(Document, score), ...]` instead of just `[Document, ...]`.

---

## 2️⃣ Keep track of `file_name`, `page`, `document_type`, and score

Inside `retrieve_with_scores`, we extract metadata and build both:

* a **`context`** string for the LLM
* a **`sources`** list for the UI

```python
context_chunks: List[str] = []
sources: List[Dict[str, Any]] = []

for i, (doc, score) in enumerate(docs_and_scores):
    file_name = doc.metadata.get("file_name", "Unknown")
    doc_type = doc.metadata.get("document_type", "Unknown")
    page = doc.metadata.get("page", "Unknown")

    # FAISS score is a distance: lower = more similar
    distance = float(score)
    confidence = 1.0 / (1.0 + distance)  # simple heuristic

    context_chunks.append(
        f"[Source {i+1} | {file_name} | type={doc_type} | page={page} | "
        f"distance={distance:.4f} | conf≈{confidence:.3f}]\n"
        f"{doc.page_content}"
    )

    sources.append(
        {
            "id": i + 1,
            "file_name": file_name,
            "document_type": doc_type,
            "page": page,
            "distance": distance,
            "confidence": confidence,
        }
    )

context = (
    "\n\n---\n\n".join(context_chunks)
    if context_chunks
    else "No relevant documents found."
)

return {
    "query": query,
    "context": context,
    "sources": sources,
}
```

Now each retrieval step returns a dict with everything the rest of the chain needs.

---

## 3️⃣ Make the LCEL chain return `{"answer": ..., "sources": [...]}`

Previously the chain likely produced just a **string answer**.

Now we map the pipeline output to a dict with **two keys**:

* `"answer"` – generated by the LLM
* `"sources"` – passed through from `retrieve_with_scores`

```python
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser

ANSWER_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are the in-house assistant for Chitown Custom Choppers..."
        ),
        (
            "human",
            "Context:\n{context}\n\nCustomer Question: {query}\n\n"
            "Answer in a helpful, concise way."
        ),
    ]
)

rag_chain = (
    {
        "query": RunnablePassthrough(),
        "classification": {"query": RunnablePassthrough()} | classification_chain,
    }
    | RunnableLambda(retrieve_with_scores)
    | {
        "answer": ANSWER_PROMPT | llm | StrOutputParser(),
        "sources": itemgetter("sources"),
    }
)
```

This is the **critical change**: the chain now returns a structured object, not just text.

---

## 4️⃣ Show answer + Sources block in Streamlit

Finally, the Streamlit UI is updated to:

1. Call the chain
2. Unpack `answer` and `sources`
3. Render sources under the answer

**Before (conceptually):**

```python
result = rag_chain.invoke(prompt_input)
st.markdown(result)  # just a string
```

**After:**

```python
result = rag_chain.invoke(prompt_input)

answer = result.get("answer", "")
sources = result.get("sources", [])

st.markdown(answer)

if sources:
    st.markdown("---")
    st.markdown("**Sources used:**")
    for s in sources:
        st.markdown(
            f"- Source {s['id']}: `{s['file_name']}` "
            f"(type: {s['document_type']}, page: {s['page']}, "
            f"distance: {s['distance']:.4f}, conf≈{s['confidence']:.3f})"
        )
```

This is what turns your RAG app from:

> “Trust me, bro, I looked stuff up.”

into:

> “Here’s the answer, plus exactly which HR / operations / marketing document and page I used.”

---

## Summary of This Upgrade

* **Retrieval step** now uses `similarity_search_with_score`
* **Metadata-aware provenance** is tracked per chunk:

  * `file_name`
  * `document_type`
  * `page`
  * `distance` (raw FAISS score)
  * `confidence` (heuristic)
* **Chain output** is now a structured dict with:

  * `"answer"`
  * `"sources"`
* **UI** displays both the answer and a human-readable **Sources** block

